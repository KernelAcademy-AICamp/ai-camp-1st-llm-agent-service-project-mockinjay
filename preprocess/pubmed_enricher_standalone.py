"""
PubMed + Semantic Scholar APIë¥¼ ì‚¬ìš©í•œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë³´ì™„ ìŠ¤í¬ë¦½íŠ¸

ì£¼ìš” íŠ¹ì§•:
- 1ì°¨: PubMedì—ì„œ ì œëª© ì™„ì „ ì¼ì¹˜ ê²€ìƒ‰ (ì ì§„ì : 5ê°œ â†’ 10ê°œ â†’ 25ê°œ)
- 2ì°¨: Semantic Scholar APIë¡œ ì œëª© ì™„ì „ ì¼ì¹˜ ê²€ìƒ‰ (ìµœëŒ€ 10ê°œ)
- ëŒ€ì†Œë¬¸ì ë° ê³µë°± ì°¨ì´ ë¬´ì‹œ
- 429 ì˜¤ë¥˜ ìë™ ì¬ì‹œë„ (ì§€ìˆ˜ ë°±ì˜¤í”„)
- ë‘ API ëª¨ë‘ ì™„ì „ ì¼ì¹˜ë§Œ í—ˆìš© (ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ ì œê±°)

Semantic Scholar API ë¬¸ì„œ:
https://api.semanticscholar.org/api-docs/

ì‚¬ìš© ë°©ë²•:
    python pubmed_with_semantic_scholar.py --max 100 --email your@email.com --s2-api-key YOUR_KEY
"""

import json
import time
import argparse
import requests
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import random
from difflib import SequenceMatcher


class SemanticScholarPubMedEnricher:
    """PubMed + Semantic Scholar API í•˜ì´ë¸Œë¦¬ë“œ ë©”íƒ€ë°ì´í„° ë³´ì™„ í´ë˜ìŠ¤"""
    
    PUBMED_BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
    S2_BASE_URL = "https://api.semanticscholar.org/graph/v1"
    
    def __init__(self, email: str = "user@example.com", 
                 pubmed_api_key: Optional[str] = None,
                 s2_api_key: Optional[str] = None,
                 delay: float = 0.5, 
                 similarity_threshold: float = 0.85):
        """
        ì´ˆê¸°í™”
        
        Args:
            email: NCBI ì´ë©”ì¼
            pubmed_api_key: NCBI API í‚¤
            s2_api_key: Semantic Scholar API í‚¤ (ì„ íƒ, ìˆìœ¼ë©´ rate limit ì¦ê°€)
            delay: ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            similarity_threshold: ì œëª© ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0)
        """
        self.email = email
        self.pubmed_api_key = pubmed_api_key
        self.s2_api_key = s2_api_key
        self.delay = delay
        self.similarity_threshold = similarity_threshold
        self.max_retries = 5
        
        # S2 API í—¤ë”
        self.s2_headers = {}
        if s2_api_key:
            self.s2_headers['x-api-key'] = s2_api_key
        
        self.stats = {
            'total': 0,
            'processed': 0,
            'pubmed_searched': 0,
            'pubmed_found': 0,
            's2_searched': 0,
            's2_found': 0,
            's2_low_similarity': 0,
            'not_found': 0,
            'already_complete': 0,
            'errors': 0,
            'rate_limit_hits': 0,
            'title_mismatch_rejected': 0,
        }
    
    @staticmethod
    def calculate_title_similarity(title1: str, title2: str) -> float:
        """ë‘ ì œëª© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)"""
        norm1 = SemanticScholarPubMedEnricher.normalize_title(title1)
        norm2 = SemanticScholarPubMedEnricher.normalize_title(title2)
        return SequenceMatcher(None, norm1, norm2).ratio()
    
    @staticmethod
    def titles_match(title1: str, title2: str) -> bool:
        """ë‘ ì œëª©ì´ ë™ì¼í•œì§€ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë° ê³µë°± ë¬´ì‹œ)"""
        norm1 = SemanticScholarPubMedEnricher.normalize_title(title1)
        norm2 = SemanticScholarPubMedEnricher.normalize_title(title2)
        return norm1 == norm2
    
    @staticmethod
    def normalize_title(title: str) -> str:
        """ì œëª© ì •ê·œí™” (ë¹„êµìš©)"""
        normalized = title.lower()
        for char in ['[', ']', '(', ')', ':', '?', '.', ',', ';', '!', '"', "'", '-']:
            normalized = normalized.replace(char, ' ')
        normalized = ' '.join(normalized.split())
        return normalized.strip()
    
    def _wait_with_backoff(self, attempt: int = 0):
        """ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ëŒ€ê¸°"""
        if attempt == 0:
            wait_time = self.delay
        else:
            wait_time = min(2 ** attempt, 30)
            wait_time += random.uniform(0, 1)
        
        if attempt > 0:
            print(f"      â³ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘... (ì¬ì‹œë„ {attempt}/{self.max_retries})")
        
        time.sleep(wait_time)
    
    # ==================== PubMed ê´€ë ¨ ë©”ì„œë“œ ====================
    
    def search_pubmed(self, query: str, max_results: int = 25) -> List[str]:
        """PubMed ê²€ìƒ‰"""
        params = {
            'db': 'pubmed',
            'term': query,
            'retmax': max_results,
            'retmode': 'json',
            'email': self.email
        }
        
        if self.pubmed_api_key:
            params['api_key'] = self.pubmed_api_key
        
        for attempt in range(self.max_retries):
            try:
                if attempt > 0:
                    self._wait_with_backoff(attempt)
                
                response = requests.get(f"{self.PUBMED_BASE_URL}esearch.fcgi", 
                                       params=params, timeout=15)
                
                if response.status_code == 429:
                    self.stats['rate_limit_hits'] += 1
                    print(f"      âš ï¸ PubMed API í˜¸ì¶œ ì œí•œ (429) - ì¬ì‹œë„ ì¤‘...")
                    continue
                
                response.raise_for_status()
                data = response.json()
                pmids = data.get('esearchresult', {}).get('idlist', [])
                return pmids
                
            except requests.exceptions.RequestException as e:
                if attempt == self.max_retries - 1:
                    print(f"      âŒ PubMed ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)[:80]}")
                    return []
        
        return []
    
    def fetch_article_metadata(self, pmid: str) -> Optional[Dict]:
        """PubMed ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        params = {
            'db': 'pubmed',
            'id': pmid,
            'retmode': 'xml',
            'email': self.email
        }
        
        if self.pubmed_api_key:
            params['api_key'] = self.pubmed_api_key
        
        for attempt in range(self.max_retries):
            try:
                if attempt > 0:
                    self._wait_with_backoff(attempt)
                
                response = requests.get(f"{self.PUBMED_BASE_URL}efetch.fcgi", 
                                       params=params, timeout=15)
                
                if response.status_code == 429:
                    self.stats['rate_limit_hits'] += 1
                    print(f"      âš ï¸ API í˜¸ì¶œ ì œí•œ (429) - ì¬ì‹œë„ ì¤‘...")
                    continue
                
                response.raise_for_status()
                root = ET.fromstring(response.content)
                article_elem = root.find('.//PubmedArticle')
                
                if article_elem is None:
                    return None
                
                return self._parse_article_xml(article_elem)
                
            except requests.exceptions.RequestException as e:
                if attempt == self.max_retries - 1:
                    print(f"      âŒ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)[:80]}")
                    return None
        
        return None
    
    def _parse_article_xml(self, article_elem) -> Optional[Dict]:
        """XMLì—ì„œ ë©”íƒ€ë°ì´í„° íŒŒì‹±"""
        try:
            medline_citation = article_elem.find('.//MedlineCitation')
            article = medline_citation.find('.//Article')
            
            # ì œëª©
            title_elem = article.find('.//ArticleTitle')
            title = ''.join(title_elem.itertext()) if title_elem is not None else ''
            
            # ì €ì
            authors = []
            for author_elem in article.findall('.//Author'):
                last_name = author_elem.findtext('LastName', '')
                fore_name = author_elem.findtext('ForeName', '')
                if last_name or fore_name:
                    authors.append(f"{fore_name} {last_name}".strip())
            
            # ì €ë„
            journal_elem = article.find('.//Journal')
            journal = journal_elem.findtext('.//Title', '') if journal_elem is not None else ''
            
            # ì¶œíŒì¼
            pub_date_elem = article.find('.//PubDate')
            publication_date = self._parse_pub_date(pub_date_elem)
            
            # DOI
            doi = ''
            for article_id in article_elem.findall('.//ArticleId'):
                if article_id.get('IdType') == 'doi':
                    doi = article_id.text
                    break
            
            # í‚¤ì›Œë“œ
            keywords = []
            for keyword_elem in medline_citation.findall('.//Keyword'):
                keyword = keyword_elem.text
                if keyword:
                    keywords.append(keyword)
            
            # MeSH ìš©ì–´ ì¶”ê°€
            mesh_count = 0
            for mesh_elem in medline_citation.findall('.//MeshHeading/DescriptorName'):
                if mesh_count >= 5:
                    break
                mesh_term = mesh_elem.text
                if mesh_term and mesh_term not in keywords:
                    keywords.append(mesh_term)
                    mesh_count += 1
            
            metadata = {
                'title': title,
                'keywords': keywords,
                'journal': journal,
                'authors': authors,
                'doi': doi,
                'publication_date': publication_date,
                'source': 'pubmed'
            }
            
            return metadata
            
        except Exception as e:
            print(f"      âš ï¸ XML íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_pub_date(self, pub_date_elem) -> str:
        """ì¶œíŒì¼ íŒŒì‹±"""
        if pub_date_elem is None:
            return ''
        
        year = pub_date_elem.findtext('Year', '')
        month = pub_date_elem.findtext('Month', '01')
        day = pub_date_elem.findtext('Day', '01')
        
        month_map = {
            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',
            'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',
            'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'
        }
        
        if month in month_map:
            month = month_map[month]
        elif not month.isdigit():
            month = '01'
        
        try:
            month = str(month).zfill(2)
            day = str(day).zfill(2)
            return f"{year}.{month}.{day}"
        except:
            return year if year else ''
    
    @staticmethod
    def prepare_search_query(title: str) -> str:
        """ì œëª©ì„ PubMed ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜"""
        clean_title = title.replace('[', '').replace(']', '')
        query = f'{clean_title.lower()[:-1]}'
        return query
    
    def find_exact_match_progressive(self, original_title: str) -> Optional[Tuple[str, Dict]]:
        """
        ì ì§„ì ìœ¼ë¡œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ëŠ˜ë ¤ê°€ë©° PubMedì—ì„œ ì™„ì „ ì¼ì¹˜ ì°¾ê¸°
        """
        query = self.prepare_search_query(original_title)
        print(f"      ğŸ” PubMed ê²€ìƒ‰: {original_title[:50]}...")
        
        queries = [f'"{query}[Title]"'] # + [f'"{query[:i]}[Title]"' for i in range(5, len(query), len(query)//5)]

        for query_variant in queries:
            search_limits = [50]
            
            for limit in search_limits:
                pmids = self.search_pubmed(query_variant, max_results=limit)
                
                if not pmids:
                    continue
                
                print(f"      ğŸ” {len(pmids)}ê°œ í›„ë³´ ë°œê²¬, ì œëª© ì™„ì „ ì¼ì¹˜ í™•ì¸ ì¤‘...")
                
                for idx, pmid in enumerate(pmids, 1):
                    metadata = self.fetch_article_metadata(pmid)
                    
                    if not metadata or 'title' not in metadata:
                        continue
                    
                    fetched_title = metadata['title']
                    if self.titles_match(original_title, fetched_title):
                        print(f"      âœ… PubMedì—ì„œ ì™„ì „ ì¼ì¹˜ ë°œê²¬! (PMID: {pmid})")
                        return (pmid, metadata)
                    else:
                        if idx <= 3:
                            print(f"      âŒ [{idx}] ì œëª© ë¶ˆì¼ì¹˜: {fetched_title[:50]}...")
                        self.stats['title_mismatch_rejected'] += 1
                    
                    time.sleep(self.delay)
                
                time.sleep(self.delay)
        
        return None
    
    # ==================== Semantic Scholar ê´€ë ¨ ë©”ì„œë“œ ====================
    
    def search_semantic_scholar(self, title: str, limit: int = 5) -> List[Dict]:
        """
        Semantic Scholar APIë¡œ ë…¼ë¬¸ ê²€ìƒ‰
        
        Args:
            title: ê²€ìƒ‰í•  ë…¼ë¬¸ ì œëª©
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        print(f"      ğŸ“ Semantic Scholar ê²€ìƒ‰ ì¤‘...")
        
        # Paper search endpoint
        endpoint = f"{self.S2_BASE_URL}/paper/search"
        
        params = {
            'query': title,
            'limit': limit,
            'fields': 'title,authors,year,venue,externalIds,publicationDate,citationCount,abstract'
        }
        
        for attempt in range(self.max_retries):
            try:
                if attempt > 0:
                    self._wait_with_backoff(attempt)
                
                response = requests.get(
                    endpoint,
                    params=params,
                    headers=self.s2_headers,
                    timeout=15
                )
                
                if response.status_code == 429:
                    self.stats['rate_limit_hits'] += 1
                    print(f"      âš ï¸ Semantic Scholar API ì œí•œ (429) - ì¬ì‹œë„ ì¤‘...")
                    continue
                
                response.raise_for_status()
                data = response.json()
                papers = data.get('data', [])
                
                print(f"      ğŸ“š {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")
                return papers
                
            except requests.exceptions.RequestException as e:
                if attempt == self.max_retries - 1:
                    print(f"      âŒ Semantic Scholar ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)[:80]}")
                    return []
        
        return []
    
    def find_best_match_semantic_scholar(self, original_title: str) -> Optional[Dict]:
        """
        Semantic Scholarì—ì„œ ì œëª© ì™„ì „ ì¼ì¹˜ ë…¼ë¬¸ ì°¾ê¸°
        
        Args:
            original_title: ì›ë³¸ ë…¼ë¬¸ ì œëª©
            
        Returns:
            ë©”íƒ€ë°ì´í„° ë˜ëŠ” None
        """
        papers = self.search_semantic_scholar(original_title, limit=10)
        
        if not papers:
            print(f"      âŒ Semantic Scholar ê²°ê³¼ ì—†ìŒ")
            return None
        
        print(f"      ğŸ” ì œëª© ì™„ì „ ì¼ì¹˜ í™•ì¸ ì¤‘...")
        
        for idx, paper in enumerate(papers, 1):
            paper_title = paper.get('title', '')
            
            if not paper_title:
                continue
            
            # ì œëª© ì™„ì „ ì¼ì¹˜ í™•ì¸
            if self.titles_match(original_title, paper_title):
                print(f"      âœ… Semantic Scholarì—ì„œ ì™„ì „ ì¼ì¹˜ ë°œê²¬! [{idx}/{len(papers)}]")
                
                # ë©”íƒ€ë°ì´í„° ë³€í™˜
                metadata = self._convert_s2_metadata(paper)
                return metadata
            else:
                if idx <= 3:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
                    similarity = self.calculate_title_similarity(original_title, paper_title)
                    print(f"      âŒ [{idx}] ì œëª© ë¶ˆì¼ì¹˜ (ìœ ì‚¬ë„ {similarity:.3f}): {paper_title[:50]}...")
        
        print(f"      âš ï¸ {len(papers)}ê°œ ì¤‘ ì™„ì „ ì¼ì¹˜í•˜ëŠ” ë…¼ë¬¸ ì—†ìŒ")
        self.stats['s2_low_similarity'] += 1
        return None
    
    def _convert_s2_metadata(self, paper: Dict) -> Dict:
        """
        Semantic Scholar ë…¼ë¬¸ ë°ì´í„°ë¥¼ í‘œì¤€ ë©”íƒ€ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            paper: Semantic Scholar API ì‘ë‹µ
            
        Returns:
            í‘œì¤€ ë©”íƒ€ë°ì´í„°
        """
        # ì €ì ì¶”ì¶œ
        authors = []
        for author in paper.get('authors', []):
            author_name = author.get('name', '')
            if author_name:
                authors.append(author_name)
        
        # DOI ì¶”ì¶œ
        external_ids = paper.get('externalIds', {})
        doi = external_ids.get('DOI', '')
        
        # PubMed IDë„ ìˆìœ¼ë©´ ì €ì¥
        pmid = external_ids.get('PubMed', '')
        
        # ì¶œíŒì¼
        pub_date = paper.get('publicationDate', '')
        if not pub_date:
            year = paper.get('year')
            pub_date = str(year) if year else ''
        
        # ì €ë„/í•™íšŒ
        venue = paper.get('venue', '')
        
        # ì´ˆë¡ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
        keywords = self._extract_keywords_from_abstract(paper.get('abstract', ''))
        
        metadata = {
            'title': paper.get('title', ''),  # ê²€ì¦ìš© ì œëª© í¬í•¨
            'keywords': keywords,
            'journal': venue,
            'authors': authors,
            'doi': doi,
            'publication_date': pub_date,
            'source': 'semantic_scholar',
            'citation_count': paper.get('citationCount', 0),
        }
        
        if pmid:
            metadata['pmid'] = pmid
        
        return metadata
    
    def _extract_keywords_from_abstract(self, abstract: str, max_keywords: int = 5) -> List[str]:
        """
        ì´ˆë¡ì—ì„œ ê°„ë‹¨íˆ í‚¤ì›Œë“œ ì¶”ì¶œ
        (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ê¸°ë²• ì‚¬ìš© ê°€ëŠ¥)
        """
        if not abstract:
            return []
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        words = abstract.lower().split()
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 
                      'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'are',
                      'was', 'were', 'been', 'be', 'have', 'has', 'had', 'this',
                      'that', 'these', 'those', 'we', 'our', 'their', 'which'}
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_freq = {}
        for word in words:
            word = word.strip('.,;:!?()[]{}')
            if len(word) > 4 and word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        keywords = [word for word, freq in sorted_words[:max_keywords]]
        
        return keywords
    
    # ==================== ë©”ì¸ ì²˜ë¦¬ ë¡œì§ ====================
    
    def enrich_article(self, article: Dict, index: int, total: int) -> Dict:
        """
        ë‹¨ì¼ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë³´ì™„
        1ì°¨: PubMed ì™„ì „ ì¼ì¹˜ ê²€ìƒ‰
        2ì°¨: Semantic Scholar semantic search
        """
        self.stats['processed'] += 1
        title = article.get('title', '')
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if index % 10 == 0 or index == 1 or index == total:
            percentage = index / total * 100
            truncated_title = title[:40] + '...' if len(title) > 40 else title
            print(f"\n[{index:,}/{total:,}] {percentage:5.1f}% - {truncated_title:45}")
        
        # ì´ë¯¸ ë©”íƒ€ë°ì´í„°ê°€ ì™„ì „í•œ ê²½ìš°
        if 'metadata' in article and article['metadata']:
            metadata = article['metadata']
            if all([metadata.get('doi'), metadata.get('journal'), 
                   metadata.get('authors'), metadata.get('publication_date')]):
                self.stats['already_complete'] += 1
                return article
        
        if not title:
            return article
        
        # 1ì°¨ ì‹œë„: PubMed ê²€ìƒ‰
        self.stats['pubmed_searched'] += 1
        match_result = self.find_exact_match_progressive(title)
        
        if match_result:
            pmid, metadata = match_result
            metadata.pop('title', None)
            article['metadata'] = metadata
            self.stats['pubmed_found'] += 1
            return article
        
        # 2ì°¨ ì‹œë„: Semantic Scholar
        print(f"      âš ï¸ PubMed ì‹¤íŒ¨, Semantic Scholar ì‹œë„...")
        self.stats['s2_searched'] += 1
        
        s2_metadata = self.find_best_match_semantic_scholar(title)
        
        if s2_metadata:
            s2_metadata.pop('title', None)  # ê²€ì¦ìš© ì œëª© ì œê±°
            article['metadata'] = s2_metadata
            self.stats['s2_found'] += 1
            return article
        
        # ë‘˜ ë‹¤ ì‹¤íŒ¨
        print(f"      âŒ ë‘ API ëª¨ë‘ ì‹¤íŒ¨")
        self.stats['not_found'] += 1
        
        return article
    
    @staticmethod
    def load_jsonl(filepath: str) -> List[Dict]:
        """JSONL íŒŒì¼ ë¡œë“œ"""
        articles = []
        errors = 0
        
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    article = json.loads(line)
                    articles.append(article)
                except json.JSONDecodeError:
                    errors += 1
        
        if errors > 0:
            print(f"âš ï¸  {errors}ê°œ ë¼ì¸ íŒŒì‹± ì˜¤ë¥˜ (ê±´ë„ˆëœ€)")
        
        return articles
    
    @staticmethod
    def save_jsonl(articles: List[Dict], filepath: str):
        """JSONL íŒŒì¼ë¡œ ì €ì¥"""
        output_file = Path(filepath)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for article in articles:
                json_line = json.dumps(article, ensure_ascii=False)
                f.write(json_line + '\n')
        
        return output_file
    
    def save_checkpoint(self, articles: List[Dict], filepath: str, current_index: int):
        """ì¤‘ê°„ ì €ì¥"""
        checkpoint_file = filepath.replace('.jsonl', f'_checkpoint_{current_index}.jsonl')
        self.save_jsonl(articles[:current_index], checkpoint_file)
        print(f"      ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_file}")
    
    def print_statistics(self):
        """ì²˜ë¦¬ í†µê³„ ì¶œë ¥"""
        print(f"\n{'='*70}")
        print("ì²˜ë¦¬ í†µê³„")
        print(f"{'='*70}")
        print(f"ì´ ë…¼ë¬¸ ìˆ˜: {self.stats['total']:,}ê°œ")
        print(f"ì²˜ë¦¬ëœ ë…¼ë¬¸: {self.stats['processed']:,}ê°œ")
        print(f"ì´ë¯¸ ì™„ì „í•œ ë©”íƒ€ë°ì´í„°: {self.stats['already_complete']:,}ê°œ")
        print()
        print("ğŸ“Š APIë³„ ê²€ìƒ‰ ê²°ê³¼:")
        print(f"  PubMed:")
        print(f"    - ê²€ìƒ‰ ì‹œë„: {self.stats['pubmed_searched']:,}ê°œ")
        print(f"    - ì„±ê³µ: {self.stats['pubmed_found']:,}ê°œ")
        print(f"    - ì œëª© ë¶ˆì¼ì¹˜ë¡œ ê±°ë¶€: {self.stats['title_mismatch_rejected']:,}ê°œ")
        print(f"  Semantic Scholar:")
        print(f"    - ê²€ìƒ‰ ì‹œë„: {self.stats['s2_searched']:,}ê°œ")
        print(f"    - ì„±ê³µ: {self.stats['s2_found']:,}ê°œ")
        print(f"    - ì™„ì „ ì¼ì¹˜ ì—†ìŒ: {self.stats['s2_low_similarity']:,}ê°œ")
        print()
        print(f"ì°¾ì§€ ëª»í•¨: {self.stats['not_found']:,}ê°œ")
        print(f"ì˜¤ë¥˜: {self.stats['errors']:,}ê°œ")
        print(f"API ì œí•œ (429) íšŸìˆ˜: {self.stats['rate_limit_hits']:,}ê°œ")
        
        total_searched = self.stats['pubmed_searched'] + self.stats['s2_searched']
        total_found = self.stats['pubmed_found'] + self.stats['s2_found']
        
        if total_searched > 0:
            success_rate = total_found / total_searched * 100
            print(f"\nì „ì²´ ê²€ìƒ‰ ì„±ê³µë¥ : {success_rate:.1f}%")
            
            if self.stats['pubmed_searched'] > 0:
                pubmed_rate = self.stats['pubmed_found'] / self.stats['pubmed_searched'] * 100
                print(f"  - PubMed ì„±ê³µë¥ : {pubmed_rate:.1f}%")
            
            if self.stats['s2_searched'] > 0:
                s2_rate = self.stats['s2_found'] / self.stats['s2_searched'] * 100
                print(f"  - Semantic Scholar ì„±ê³µë¥ : {s2_rate:.1f}%")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='PubMed + Semantic Scholar API í•˜ì´ë¸Œë¦¬ë“œ ë©”íƒ€ë°ì´í„° ë³´ì™„ ë„êµ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
íŠ¹ì§•:
  - 1ì°¨: PubMed ì œëª© ì™„ì „ ì¼ì¹˜ ê²€ìƒ‰ (ì ì§„ì : 5â†’10â†’25ê°œ)
  - 2ì°¨: Semantic Scholar API ì œëª© ì™„ì „ ì¼ì¹˜ ê²€ìƒ‰ (ìµœëŒ€ 10ê°œ)
  - ëŒ€ì†Œë¬¸ì ë° ê³µë°± ì°¨ì´ ë¬´ì‹œ
  - ë‘ API ëª¨ë‘ ì™„ì „ ì¼ì¹˜ë§Œ í—ˆìš©
  
Semantic Scholar API:
  - ë¬´ë£Œ tier: 100 requests/5min
  - API key ì‚¬ìš© ì‹œ: ë” ë†’ì€ rate limit
  - ë¬¸ì„œ: https://api.semanticscholar.org/
        """
    )
    parser.add_argument('--max', type=int, default=None,
                       help='ì²˜ë¦¬í•  ìµœëŒ€ ë…¼ë¬¸ ìˆ˜')
    parser.add_argument('--input', type=str,
                       default='/Users/jaehuncho/Coding/ai-camp-1st-llm-agent-service-project-mockinjay/data/preprocess/unified_output/paper_dataset_.jsonl',
                       help='ì…ë ¥ JSONL íŒŒì¼')
    parser.add_argument('--output', type=str,
                       default='/Users/jaehuncho/Coding/ai-camp-1st-llm-agent-service-project-mockinjay/data/preprocess/unified_output/paper_dataset_final.jsonl',
                       help='ì¶œë ¥ JSONL íŒŒì¼')
    parser.add_argument('--email', type=str,
                       default='ggh5454@gmail.com',
                       help='ì´ë©”ì¼ ì£¼ì†Œ (NCBI ìš”êµ¬ì‚¬í•­)')
    parser.add_argument('--pubmed-api-key', type=str, default=None,
                       help='NCBI API í‚¤ (ì„ íƒ)')
    parser.add_argument('--s2-api-key', type=str, default=None,
                       help='Semantic Scholar API í‚¤ (ì„ íƒ, rate limit ì¦ê°€)')
    parser.add_argument('--delay', type=float, default=0.5,
                       help='API í˜¸ì¶œ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)')
    parser.add_argument('--similarity-threshold', type=float, default=0.85,
                       help='[ì‚¬ìš©ì•ˆí•¨] í˜„ì¬ëŠ” ì œëª© ì™„ì „ ì¼ì¹˜ë§Œ ì‚¬ìš©')
    parser.add_argument('--checkpoint-interval', type=int, default=50,
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²©')
    
    args = parser.parse_args()
    
    print("="*70)
    print("PubMed + Semantic Scholar API í•˜ì´ë¸Œë¦¬ë“œ ë©”íƒ€ë°ì´í„° ë³´ì™„ ë„êµ¬")
    print("="*70)
    print(f"ì…ë ¥ íŒŒì¼: {args.input}")
    print(f"ì¶œë ¥ íŒŒì¼: {args.output}")
    print(f"ì´ë©”ì¼: {args.email}")
    print(f"API í˜¸ì¶œ ê°„ê²©: {args.delay}ì´ˆ")
    print(f"S2 ìœ ì‚¬ë„ ì„ê³„ê°’: {args.similarity_threshold}")
    print(f"ì²´í¬í¬ì¸íŠ¸ ê°„ê²©: {args.checkpoint_interval}ê°œë§ˆë‹¤")
    print(f"ê²€ìƒ‰ ì „ëµ: PubMed (ì™„ì „ ì¼ì¹˜) â†’ Semantic Scholar (ì™„ì „ ì¼ì¹˜)")
    if args.s2_api_key:
        print(f"Semantic Scholar API í‚¤: ì„¤ì •ë¨ (ë†’ì€ rate limit)")
    else:
        print(f"Semantic Scholar API í‚¤: ì—†ìŒ (ë¬´ë£Œ tier, 100 req/5min)")
    print(f"âš ï¸  ë‘ API ëª¨ë‘ ì œëª© ì™„ì „ ì¼ì¹˜ë§Œ í—ˆìš© (ìœ ì‚¬ ì œëª© ì œì™¸)")
    if args.max:
        print(f"ì²˜ë¦¬í•  ë…¼ë¬¸ ìˆ˜: {args.max:,}ê°œ")
    print()
    
    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    enricher = SemanticScholarPubMedEnricher(
        email=args.email, 
        pubmed_api_key=args.pubmed_api_key,
        s2_api_key=args.s2_api_key,
        delay=args.delay,
        similarity_threshold=args.similarity_threshold
    )
    
    # 1. íŒŒì¼ ë¡œë“œ
    print("ğŸ“‚ íŒŒì¼ ë¡œë”© ì¤‘...")
    articles = enricher.load_jsonl(args.input)
    print(f"âœ“ {len(articles):,}ê°œì˜ ë…¼ë¬¸ ë¡œë“œ ì™„ë£Œ\n")
    
    if not articles:
        print("âŒ ë…¼ë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if args.max:
        articles = articles[:args.max]
        print(f"â„¹ï¸  ì²˜ìŒ {args.max:,}ê°œ ë…¼ë¬¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")
    
    enricher.stats['total'] = len(articles)
    
    # 2. ë©”íƒ€ë°ì´í„° ë³´ì™„
    print(f"{'='*70}")
    print("ë©”íƒ€ë°ì´í„° ë³´ì™„ ì‹œì‘ (PubMed + Semantic Scholar)")
    print(f"{'='*70}")
    print("ğŸ’¡ íŒ: Ctrl+Cë¡œ ì•ˆì „í•˜ê²Œ ì¤‘ë‹¨ ê°€ëŠ¥")
    print()
    
    enriched_articles = []
    
    try:
        for i, article in enumerate(articles, 1):
            try:
                enriched = enricher.enrich_article(article, i, len(articles))
                enriched_articles.append(enriched)
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if i % args.checkpoint_interval == 0:
                    enricher.save_checkpoint(enriched_articles, args.output, i)
                
                # API í˜¸ì¶œ ì œí•œ ì¤€ìˆ˜
                if i < len(articles):
                    time.sleep(enricher.delay)
                    
            except KeyboardInterrupt:
                print(f"\n\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨ (Ctrl+C)")
                print(f"í˜„ì¬ê¹Œì§€ ì²˜ë¦¬: {i}/{len(articles)}")
                break
                
            except Exception as e:
                print(f"[{i}/{len(articles)}] âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                enricher.stats['errors'] += 1
                enriched_articles.append(article)
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  í”„ë¡œê·¸ë¨ ì¤‘ë‹¨ë¨")
    
    # 3. ìµœì¢… ì €ì¥
    print(f"\nğŸ’¾ ìµœì¢… íŒŒì¼ ì €ì¥ ì¤‘...")
    output_path = enricher.save_jsonl(enriched_articles, args.output)
    file_size = output_path.stat().st_size
    
    print(f"âœ“ ì €ì¥ ì™„ë£Œ")
    print(f"  ê²½ë¡œ: {output_path}")
    print(f"  í¬ê¸°: {file_size:,} bytes ({file_size / (1024*1024):.2f} MB)")
    
    # 4. í†µê³„ ì¶œë ¥
    enricher.print_statistics()
    
if __name__ == "__main__":
    main()