"""
Nutrition PDF Processor - 19ê°œ ì˜ì–‘ PDFë¥¼ Pineconeì— ì—…ë¡œë“œ
í•œê¸€ ë ˆì‹œí”¼ PDF íŒŒì‹± ë° ë²¡í„° DB êµ¬ì¶•
"""

import os
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any
import json
import time
import hashlib

# Add backend to path
backend_path = Path(__file__).parent.parent
sys.path.insert(0, str(backend_path))

import fitz  # PyMuPDF for PDF parsing
from openai import OpenAI
from dotenv import load_dotenv
from rag.nutrition_rag import NutritionRAG

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NutritionPDFProcessor:
    """PDF íŒŒì‹± ë° Pinecone ì—…ë¡œë“œ í”„ë¡œì„¸ì„œ"""

    def __init__(self):
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.rag = NutritionRAG()
        self.data_dir = Path(__file__).parent.parent.parent / "data" / "raw" / "nutri"

    def extract_text_from_pdf(self, pdf_path: Path, max_pages: int = 50) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            doc = fitz.open(str(pdf_path))
            text_chunks = []

            # Limit pages to avoid too much text
            num_pages = min(len(doc), max_pages)

            for page_num in range(num_pages):
                page = doc[page_num]
                text = page.get_text()
                if text.strip():
                    text_chunks.append(text)

            doc.close()
            return "\n\n".join(text_chunks)

        except Exception as e:
            logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ {pdf_path.name}: {e}")
            return ""

    def parse_recipes_with_openai(self, pdf_text: str, pdf_name: str) -> List[Dict[str, Any]]:
        """OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ PDFì—ì„œ ë ˆì‹œí”¼ ì¶”ì¶œ"""

        # Truncate text if too long (limit to ~15000 chars = ~4000 tokens)
        if len(pdf_text) > 15000:
            pdf_text = pdf_text[:15000] + "..."

        prompt = f"""ë‹¤ìŒì€ ì‹ ì¥ë³‘ í™˜ìë¥¼ ìœ„í•œ ì˜ì–‘ ê°€ì´ë“œ PDFì˜ ë‚´ìš©ì…ë‹ˆë‹¤.

PDF ì´ë¦„: {pdf_name}

í…ìŠ¤íŠ¸ ë‚´ìš©:
{pdf_text}

ìœ„ ë‚´ìš©ì—ì„œ ë ˆì‹œí”¼/ìš”ë¦¬ë¥¼ ì°¾ì•„ì„œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ìµœëŒ€ 20ê°œì˜ ë ˆì‹œí”¼ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ê° ë ˆì‹œí”¼ì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
- dish_name: ìš”ë¦¬ëª… (í•œê¸€)
- ingredients: ì¬ë£Œ ë¦¬ìŠ¤íŠ¸ (ë°°ì—´)
- recipe: ì¡°ë¦¬ë²• (ìƒì„¸í•˜ê²Œ)
- nutrition: ì˜ì–‘ì†Œ ì •ë³´
  - sodium: ë‚˜íŠ¸ë¥¨ (mg)
  - potassium: ì¹¼ë¥¨ (mg)
  - phosphorus: ì¸ (mg)
  - protein: ë‹¨ë°±ì§ˆ (g)
  - calcium: ì¹¼ìŠ˜ (mg)

ì˜ì–‘ì†Œ ì •ë³´ê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš°, ì¼ë°˜ì ì¸ 1ì¸ë¶„ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì •ê°’ì„ ì œê³µí•˜ì„¸ìš”.
ì‹ ì¥ë³‘ í™˜ìë¥¼ ìœ„í•œ ì €ì—¼, ì €ì¹¼ë¥¨, ì €ì¸ ë ˆì‹œí”¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

ì‘ë‹µì€ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì œê³µí•˜ì„¸ìš”:
```json
{{
  "recipes": [
    {{
      "dish_name": "ìš”ë¦¬ëª…",
      "ingredients": ["ì¬ë£Œ1", "ì¬ë£Œ2", "ì¬ë£Œ3"],
      "recipe": "1. ë‹¨ê³„1\\n2. ë‹¨ê³„2\\n3. ë‹¨ê³„3",
      "nutrition": {{
        "sodium": 350,
        "potassium": 400,
        "phosphorus": 180,
        "protein": 20,
        "calcium": 50
      }}
    }}
  ]
}}
```

JSONë§Œ ë°˜í™˜í•˜ê³  ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹ ì¥ë³‘ í™˜ìë¥¼ ìœ„í•œ ì˜ì–‘ ë ˆì‹œí”¼ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. PDF í…ìŠ¤íŠ¸ì—ì„œ ë ˆì‹œí”¼ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=4000
            )

            content = response.choices[0].message.content.strip()

            # Extract JSON from markdown code blocks if present
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            data = json.loads(content)
            return data.get("recipes", [])

        except Exception as e:
            logger.error(f"OpenAI ë ˆì‹œí”¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    def process_single_pdf(self, pdf_path: Path) -> int:
        """ë‹¨ì¼ PDF ì²˜ë¦¬ ë° Pinecone ì—…ë¡œë“œ"""
        logger.info(f"ğŸ“„ Processing: {pdf_path.name}")

        # Extract text
        pdf_text = self.extract_text_from_pdf(pdf_path, max_pages=50)
        if not pdf_text:
            logger.warning(f"âš ï¸ No text extracted from {pdf_path.name}")
            return 0

        logger.info(f"ğŸ“ Extracted {len(pdf_text)} characters")

        # Parse recipes with OpenAI
        recipes = self.parse_recipes_with_openai(pdf_text, pdf_path.name)
        logger.info(f"ğŸ½ï¸ Found {len(recipes)} recipes")

        # Upload to Pinecone
        uploaded = 0

        # Generate ASCII-only prefix from PDF filename using hash
        pdf_hash = hashlib.md5(pdf_path.name.encode('utf-8')).hexdigest()[:8]

        for idx, recipe in enumerate(recipes):
            try:
                # Use hash-based ID (ASCII only, no Korean)
                food_id = f"recipe_{pdf_hash}_{idx}"

                self.rag.upsert_food(
                    food_id=food_id,
                    dish_name=recipe.get("dish_name", "Unknown"),
                    ingredients=recipe.get("ingredients", []),
                    recipe=recipe.get("recipe", ""),
                    nutrition=recipe.get("nutrition", {}),
                    image=None  # No images from PDF
                )

                uploaded += 1

            except Exception as e:
                logger.error(f"âŒ Upload failed for recipe {idx}: {e}")

        logger.info(f"âœ… Uploaded {uploaded}/{len(recipes)} recipes from {pdf_path.name}")
        return uploaded

    def process_all_pdfs(self):
        """ëª¨ë“  PDF ì²˜ë¦¬"""
        pdf_files = list(self.data_dir.glob("*.pdf"))
        logger.info(f"ğŸ” Found {len(pdf_files)} PDF files in {self.data_dir}")

        if not pdf_files:
            logger.error(f"âŒ No PDF files found in {self.data_dir}")
            return

        total_uploaded = 0

        for pdf_path in pdf_files:
            try:
                uploaded = self.process_single_pdf(pdf_path)
                total_uploaded += uploaded

                # Rate limiting - wait between PDFs to avoid API rate limits
                time.sleep(2)

            except Exception as e:
                logger.error(f"âŒ Failed to process {pdf_path.name}: {e}")

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ‰ Processing complete!")
        logger.info(f"ğŸ“¦ Total recipes uploaded: {total_uploaded}")
        logger.info(f"ğŸ“š Total PDFs processed: {len(pdf_files)}")
        logger.info(f"{'='*60}\n")

        # Verify Pinecone index stats
        self.verify_upload()

    def verify_upload(self):
        """Pinecone ì—…ë¡œë“œ ê²€ì¦"""
        try:
            if self.rag.index:
                stats = self.rag.index.describe_index_stats()
                logger.info(f"âœ… Pinecone Index Stats:")
                logger.info(f"   - Index name: nutrition-ckd")
                logger.info(f"   - Total vectors: {stats.total_vector_count}")
                logger.info(f"   - Dimension: {stats.dimension}")
            else:
                logger.warning("âš ï¸ Pinecone index not available for verification")
        except Exception as e:
            logger.error(f"âŒ Verification failed: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("="*60)
    logger.info("ğŸš€ Nutrition PDF Processor - Starting")
    logger.info("="*60)

    processor = NutritionPDFProcessor()
    processor.process_all_pdfs()


if __name__ == "__main__":
    main()
