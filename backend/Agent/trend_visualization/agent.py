import sys
from pathlib import Path
from typing import Dict, Any, Optional, List, TypedDict, Tuple
import logging
from datetime import datetime
import os
import asyncio
import json
from openai import AsyncOpenAI

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
backend_path = Path(__file__).parent.parent.parent
if str(backend_path) not in sys.path:
    sys.path.insert(0, str(backend_path))

from Agent.core.local_agent import LocalAgent
from Agent.core.agent_registry import AgentRegistry
from Agent.core.contracts import AgentRequest, AgentResponse
from Agent.api.pubmed_client import PubMedClient

# LangGraph imports
from langgraph.graph import StateGraph, END

logger = logging.getLogger(__name__)


# ============================================================================
# State Definition
# ============================================================================

class AgentState(TypedDict):
    """LangGraph State for Trend Visualization Agent"""
    # ì…ë ¥
    query: str
    session_id: str
    context: Dict[str, Any]
    
    # ë¶„ì„ ê²°ê³¼
    analysis_type: str  # temporal, geographic, mesh, compare, general
    keywords: List[str]
    
    # PubMed ë°ì´í„°
    pubmed_data: Optional[Dict[str, Any]]
    papers: List[Dict[str, Any]]
    
    # ì‹œê°í™” ë°ì´í„°
    chart_config: Optional[Dict[str, Any]]
    
    # ì‘ë‹µ
    explanation: str
    status: str
    error: Optional[str]
    
    # ë©”íƒ€ë°ì´í„° (operator.add ì œê±°)
    metadata: Dict[str, Any]


# ============================================================================
# LangGraph Agent
# ============================================================================

@AgentRegistry.register("trend_visualization")
class TrendVisualizationAgent(LocalAgent):
    """PubMed ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„ ì—ì´ì „íŠ¸ (LangGraph)"""

    def __init__(self):
        super().__init__(agent_type="trend_visualization")
        self.pubmed = PubMedClient()
        self._initialized = False
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # LangGraph workflow êµ¬ì„±
        self.workflow = self._build_workflow()
    
    @property
    def metadata(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ë©”íƒ€ë°ì´í„°"""
        return {
            "name": "Trend Visualization Agent (LangGraph)",
            "description": "PubMed ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„ ë° ì‹œê°í™”",
            "version": "3.0",
            "capabilities": [
                "temporal_trends",
                "geographic_distribution",
                "mesh_categories",
                "keyword_comparison",
                "data_visualization",
                "pubmed_integration"
            ],
            "data_sources": ["PubMed"],
            "workflow_engine": "LangGraph"
        }
    
    def _build_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        workflow = StateGraph(AgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyze_request", self._analyze_request)
        workflow.add_node("fetch_pubmed_data", self._fetch_pubmed_data)
        workflow.add_node("generate_visualization", self._generate_visualization)
        workflow.add_node("generate_explanation", self._generate_explanation)
        
        # ì—£ì§€ ì„¤ì •
        workflow.set_entry_point("analyze_request")
        workflow.add_edge("analyze_request", "fetch_pubmed_data")
        workflow.add_edge("fetch_pubmed_data", "generate_visualization")
        workflow.add_edge("generate_visualization", "generate_explanation")
        workflow.add_edge("generate_explanation", END)
        
        return workflow.compile()

    async def _chat_completion(
        self,
        messages: List[Dict[str, Any]],
        temperature: float = 0.2,
        max_tokens: Optional[int] = None
    ) -> str:
        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content or ""

    async def process(self, request: AgentRequest) -> AgentResponse:
        """
        í†µì¼ëœ ê³„ì•½ ê¸°ë°˜ ì²˜ë¦¬ (LangGraph ì‹¤í–‰)
        
        Args:
            request: AgentRequest
            
        Returns:
            AgentResponse: í†µì¼ëœ ì‘ë‹µ í˜•ì‹
        """
        try:
            # ì´ˆê¸° ìƒíƒœ ìƒì„±
            initial_state: AgentState = {
                "query": request.query,
                "session_id": request.session_id,
                "context": request.context or {},
                "analysis_type": "",
                "keywords": [],
                "pubmed_data": None,
                "papers": [],
                "chart_config": None,
                "explanation": "",
                "status": "processing",
                "error": None,
                "metadata": {}
            }
            
            # LangGraph ì‹¤í–‰
            logger.info(f"ğŸš€ Starting LangGraph workflow for query: {request.query}")
            final_state = await self.workflow.ainvoke(initial_state)
            
            # ì‘ë‹µ ìƒì„±
            if final_state.get("error"):
                return AgentResponse(
                    answer=f"íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {final_state['error']}",
                    sources=[],
                    papers=[],
                    tokens_used=100,
                    status="error",
                    agent_type=self.agent_type,
                    metadata=final_state.get("metadata", {})
                )
            
            return AgentResponse(
                answer=final_state.get("explanation", ""),
                sources=[final_state.get("chart_config")] if final_state.get("chart_config") else [],
                papers=final_state.get("papers", [])[:5],
                tokens_used=200,
                status=final_state.get("status", "success"),
                agent_type=self.agent_type,
                metadata=final_state.get("metadata", {})
            )
            
        except Exception as e:
            logger.error(f"âŒ LangGraph workflow error: {e}", exc_info=True)
            return AgentResponse(
                answer=f"íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                sources=[],
                papers=[],
                tokens_used=0,
                status="error",
                agent_type=self.agent_type,
                metadata={"error": str(e)}
            )

    # ========================================================================
    # LangGraph Nodes
    # ========================================================================

    async def _analyze_request(self, state: AgentState) -> AgentState:
        """
        Node 1: ìš”ì²­ ë¶„ì„ ë° ë¶„ì„ íƒ€ì… ê²°ì •
        """
        logger.info("ğŸ“Š Node 1: Analyzing request...")
        
        query = state["query"]
        context = state.get("context", {})
        
        # ë¶„ì„ íƒ€ì… ê²°ì •
        analysis_type = context.get("analysisType", "temporal_trends")
        
        # ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = context.get("keywords", [query])
        if not keywords:
            keywords = [query]
        
        logger.info(f"   Analysis type: {analysis_type}")
        logger.info(f"   Keywords: {keywords}")
        
        state["analysis_type"] = analysis_type
        state["keywords"] = keywords
        state["metadata"]["analysis_type"] = analysis_type
        
        return state

    async def _fetch_pubmed_data(self, state: AgentState) -> AgentState:
        """
        Node 2: PubMedì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        """
        logger.info("ğŸ” Node 2: Fetching PubMed data...")
        
        try:
            analysis_type = state["analysis_type"]
            keywords = state["keywords"]
            context = state.get("context", {})
            
            pubmed_data = {}
            papers = []
            
            if analysis_type == "temporal_trends":
                # ì‹œê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„
                start_year = context.get("start_year", 2015)
                end_year = context.get("end_year", 2024)
                
                logger.info(f"   Fetching temporal trends ({start_year}-{end_year})...")
                
                try:
                    trends_data = await self.pubmed.searcher.get_publication_trends_parallel(
                        query=keywords[0],
                        start_year=start_year,
                        end_year=end_year,
                        normalize=True
                    )
                    
                    pubmed_data["trends"] = trends_data
                    pubmed_data["start_year"] = start_year
                    pubmed_data["end_year"] = end_year
                except Exception as e:
                    logger.error(f"   âš ï¸ Trends fetch failed: {e}")
                    state["error"] = f"ì‹œê°„ë³„ íŠ¸ë Œë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (Rate Limit ê°€ëŠ¥ì„±)"
                    state["status"] = "error"
                    return state
                
                # ìµœê·¼ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸° (Rate Limit íšŒí”¼)
                try:
                    papers = await self.pubmed.search(
                        query=keywords[0],
                        max_results=5,  # 10 â†’ 5ë¡œ ì¤„ì„
                        sort="pub_date"
                    )
                except Exception as e:
                    logger.warning(f"   âš ï¸ Papers fetch failed (Rate Limit): {e}")
                    papers = []  # ë…¼ë¬¸ ì—†ì´ ì§„í–‰
                
            elif analysis_type == "geographic_distribution":
                # ì§€ì—­ë³„ ë¶„í¬ ë¶„ì„
                countries = context.get("countries", None)
                
                logger.info(f"   Fetching geographic distribution...")
                
                try:
                    geo_data = await self.pubmed.searcher.get_geographic_distribution_parallel(
                        query=keywords[0],
                        countries=countries
                    )
                    
                    pubmed_data["geographic"] = geo_data
                except Exception as e:
                    logger.error(f"   âš ï¸ Geographic fetch failed: {e}")
                    state["error"] = "ì§€ì—­ë³„ ë¶„í¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨"
                    state["status"] = "error"
                    return state
                
                try:
                    papers = await self.pubmed.search(
                        query=keywords[0],
                        max_results=5
                    )
                except Exception as e:
                    logger.warning(f"   âš ï¸ Papers fetch failed: {e}")
                    papers = []
                
            elif analysis_type == "keyword_comparison":
                # í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„
                start_year = context.get("start_year", 2015)
                end_year = context.get("end_year", 2024)
                
                logger.info(f"   Comparing keywords: {keywords[:4]}")
                
                all_trends = []
                for keyword in keywords[:4]:  # ìµœëŒ€ 4ê°œ
                    try:
                        trends = await self.pubmed.searcher.get_publication_trends_parallel(
                            query=keyword,
                            start_year=start_year,
                            end_year=end_year,
                            normalize=True
                        )
                        all_trends.append({
                            "keyword": keyword,
                            "data": trends
                        })
                    except Exception as e:
                        logger.warning(f"   âš ï¸ Skipping keyword '{keyword}': {e}")
                        continue
                
                if not all_trends:
                    state["error"] = "í‚¤ì›Œë“œ ë¹„êµ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨"
                    state["status"] = "error"
                    return state
                
                pubmed_data["comparisons"] = all_trends
                pubmed_data["start_year"] = start_year
                pubmed_data["end_year"] = end_year
                
                # ì²« ë²ˆì§¸ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ ê²€ìƒ‰
                try:
                    papers = await self.pubmed.search(
                        query=keywords[0],
                        max_results=5
                    )
                except Exception as e:
                    logger.warning(f"   âš ï¸ Papers fetch failed: {e}")
                    papers = []
            
            else:
                # ê¸°ë³¸: ìµœê·¼ ë…¼ë¬¸ ê²€ìƒ‰
                logger.info(f"   Fetching recent papers...")
                try:
                    papers = await self.pubmed.search(
                        query=keywords[0],
                        max_results=10
                    )
                    pubmed_data["papers_count"] = len(papers)
                except Exception as e:
                    logger.error(f"   âŒ Papers fetch failed: {e}")
                    state["error"] = "ë…¼ë¬¸ ê²€ìƒ‰ ì‹¤íŒ¨"
                    state["status"] = "error"
                    return state
            
            logger.info(f"   âœ… Fetched {len(papers)} papers")
            
            state["pubmed_data"] = pubmed_data
            state["papers"] = papers
            state["metadata"]["papers_count"] = len(papers)
            state["status"] = "data_fetched"
            
        except Exception as e:
            logger.error(f"   âŒ PubMed fetch error: {e}")
            state["error"] = f"PubMed ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}"
            state["status"] = "error"
        
        return state

    async def _generate_visualization(self, state: AgentState) -> AgentState:
        """
        Node 3: ì°¨íŠ¸ ì„¤ì • ìƒì„±
        """
        logger.info("ğŸ“ˆ Node 3: Generating visualization...")
        
        # ì—ëŸ¬ ìƒíƒœë©´ ìŠ¤í‚µ
        if state.get("error"):
            logger.warning("   âš ï¸ Skipping due to previous error")
            return state
        
        try:
            analysis_type = state["analysis_type"]
            pubmed_data = state.get("pubmed_data")
            
            # pubmed_dataê°€ Noneì´ë©´ ìŠ¤í‚µ
            if not pubmed_data:
                logger.warning("   âš ï¸ No PubMed data available")
                return state
            
            chart_config = None
            
            if analysis_type == "temporal_trends" and "trends" in pubmed_data:
                # ì‹œê°„ë³„ íŠ¸ë Œë“œ ì°¨íŠ¸
                trends = pubmed_data["trends"]
                
                chart_config = {
                    "type": "line",
                    "data": {
                        "labels": [str(year) for year in trends["years"]],
                        "datasets": [
                            {
                                "label": "ë…¼ë¬¸ ìˆ˜",
                                "data": trends["counts"],
                                "borderColor": "rgb(59, 130, 246)",
                                "backgroundColor": "rgba(59, 130, 246, 0.1)",
                                "tension": 0.3
                            }
                        ]
                    },
                    "options": {
                        "responsive": True,
                        "plugins": {
                            "title": {
                                "display": True,
                                "text": "ì‹œê°„ë³„ ì—°êµ¬ íŠ¸ë Œë“œ"
                            }
                        }
                    }
                }
                
                # ì •ê·œí™” ë°ì´í„° ì¶”ê°€
                if "normalized_counts" in trends:
                    chart_config["data"]["datasets"].append({
                        "label": "ì •ê·œí™”ëœ ìˆ˜ (per 100K)",
                        "data": trends["normalized_counts"],
                        "borderColor": "rgb(239, 68, 68)",
                        "backgroundColor": "rgba(239, 68, 68, 0.1)",
                        "tension": 0.3,
                        "yAxisID": "y1"
                    })
                
            elif analysis_type == "geographic_distribution" and "geographic" in pubmed_data:
                # ì§€ì—­ë³„ ë¶„í¬ ì°¨íŠ¸
                geo_data = pubmed_data["geographic"]
                
                sorted_countries = sorted(
                    geo_data["countries"].items(),
                    key=lambda x: x[1]["count"],
                    reverse=True
                )[:15]
                
                chart_config = {
                    "type": "bar",
                    "data": {
                        "labels": [country for country, _ in sorted_countries],
                        "datasets": [{
                            "label": "ë…¼ë¬¸ ìˆ˜",
                            "data": [data["count"] for _, data in sorted_countries],
                            "backgroundColor": "rgba(59, 130, 246, 0.7)",
                            "borderColor": "rgb(59, 130, 246)",
                            "borderWidth": 1
                        }]
                    },
                    "options": {
                        "indexAxis": "y",
                        "responsive": True
                    }
                }
                
            elif analysis_type == "keyword_comparison" and "comparisons" in pubmed_data:
                # í‚¤ì›Œë“œ ë¹„êµ ì°¨íŠ¸
                comparisons = pubmed_data["comparisons"]
                
                colors = [
                    'rgb(59, 130, 246)',
                    'rgb(239, 68, 68)',
                    'rgb(34, 197, 94)',
                    'rgb(234, 179, 8)'
                ]
                
                datasets = []
                for i, comp in enumerate(comparisons):
                    datasets.append({
                        "label": comp["keyword"],
                        "data": comp["data"]["normalized_counts"],
                        "borderColor": colors[i % len(colors)],
                        "backgroundColor": colors[i % len(colors)].replace('rgb', 'rgba').replace(')', ', 0.1)'),
                        "tension": 0.3
                    })
                
                chart_config = {
                    "type": "line",
                    "data": {
                        "labels": [str(year) for year in comparisons[0]["data"]["years"]],
                        "datasets": datasets
                    },
                    "options": {
                        "responsive": True,
                        "plugins": {
                            "title": {
                                "display": True,
                                "text": "í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„"
                            }
                        }
                    }
                }
            
            if chart_config:
                logger.info(f"   âœ… Generated {chart_config['type']} chart")
                state["chart_config"] = chart_config
                state["metadata"]["chart_type"] = chart_config["type"]
            else:
                logger.info("   â„¹ï¸ No chart generated")
            
        except Exception as e:
            logger.error(f"   âŒ Visualization error: {e}")
            # ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
        
        return state

    async def _generate_explanation(self, state: AgentState) -> AgentState:
        """
        Node 4: ë¶„ì„ ê²°ê³¼ ì„¤ëª… ìƒì„±
        """
        logger.info("ğŸ’¬ Node 4: Generating explanation...")
        
        # ì—ëŸ¬ ìƒíƒœë©´ ìŠ¤í‚µ
        if state.get("error"):
            logger.warning("   âš ï¸ Using error message as explanation")
            state["explanation"] = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {state['error']}"
            return state
        
        try:
            analysis_type = state["analysis_type"]
            pubmed_data = state.get("pubmed_data")
            papers = state.get("papers", [])
            query = state["query"]
            
            # pubmed_dataê°€ Noneì´ë©´ ê¸°ë³¸ ë©”ì‹œì§€
            if not pubmed_data:
                state["explanation"] = f"""ğŸ” ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„

**ë¶„ì„ ì£¼ì œ**: "{query}"

âš ï¸ PubMed API Rate Limitìœ¼ë¡œ ì¸í•´ ì¼ì‹œì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."""
                state["status"] = "partial_success"
                return state
            
            explanation = ""
            
            if analysis_type == "temporal_trends" and "trends" in pubmed_data:
                trends = pubmed_data["trends"]
                total_papers = sum(trends["counts"])
                max_idx = trends["counts"].index(max(trends["counts"]))
                peak_year = trends["years"][max_idx]
                peak_count = trends["counts"][max_idx]
                start_year = pubmed_data["start_year"]
                end_year = pubmed_data["end_year"]
                
                explanation = f"""ğŸ“Š ì‹œê°„ë³„ ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„ ({start_year}-{end_year})

ğŸ” **ë¶„ì„ ì£¼ì œ**: "{query}"

ğŸ“ˆ **ì£¼ìš” í†µê³„**:
â€¢ ì „ì²´ ë…¼ë¬¸ ìˆ˜: {total_papers:,}ê°œ
â€¢ ìµœê³  ë°œí–‰ ì—°ë„: {peak_year}ë…„ ({peak_count:,}ê°œ)
â€¢ ë¶„ì„ ê¸°ê°„: {end_year - start_year + 1}ë…„

ğŸ’¡ **íŠ¸ë Œë“œ ìš”ì•½**:
ìµœê·¼ {end_year - start_year + 1}ë…„ê°„ "{query}" ì£¼ì œì˜ ì—°êµ¬ëŠ” ê¾¸ì¤€í•œ ê´€ì‹¬ì„ ë°›ê³  ìˆìœ¼ë©°,
{peak_year}ë…„ì— ê°€ì¥ ë§ì€ ë…¼ë¬¸ì´ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤.

ğŸ“„ ìµœê·¼ ì£¼ìš” ë…¼ë¬¸ {len(papers)}ê°œê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤."""

            elif analysis_type == "geographic_distribution" and "geographic" in pubmed_data:
                geo_data = pubmed_data["geographic"]
                sorted_countries = sorted(
                    geo_data["countries"].items(),
                    key=lambda x: x[1]["count"],
                    reverse=True
                )
                top_country = sorted_countries[0][0] if sorted_countries else "N/A"
                top_count = sorted_countries[0][1]["count"] if sorted_countries else 0
                total_results = geo_data["total_results"]
                
                explanation = f"""ğŸŒ ì§€ì—­ë³„ ì—°êµ¬ ë¶„í¬ ë¶„ì„

ğŸ” **ë¶„ì„ ì£¼ì œ**: "{query}"

ğŸ“ˆ **ì£¼ìš” í†µê³„**:
â€¢ ì´ ë…¼ë¬¸ ìˆ˜: {total_results:,}ê°œ
â€¢ ìµœë‹¤ ì—°êµ¬ êµ­ê°€: {top_country} ({top_count:,}ê°œ, {top_count/total_results*100:.1f}%)
â€¢ ë¶„ì„ êµ­ê°€ ìˆ˜: {len(sorted_countries)}ê°œ

ğŸ’¡ **ë¶„í¬ ìš”ì•½**:
"{query}" ì£¼ì œëŠ” ì „ ì„¸ê³„ì ìœ¼ë¡œ ì—°êµ¬ë˜ê³  ìˆìœ¼ë©°,
{top_country}ì—ì„œ ê°€ì¥ í™œë°œí•œ ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤."""

            elif analysis_type == "keyword_comparison" and "comparisons" in pubmed_data:
                comparisons = pubmed_data["comparisons"]
                keyword_summaries = []
                for comp in comparisons:
                    total = sum(comp["data"]["counts"])
                    keyword_summaries.append(f"â€¢ {comp['keyword']}: {total:,}ê°œ")
                
                start_year = pubmed_data["start_year"]
                end_year = pubmed_data["end_year"]
                
                explanation = f"""ğŸ“Š í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„ ({start_year}-{end_year})

ğŸ” **ë¹„êµ í‚¤ì›Œë“œ**: {len(comparisons)}ê°œ

ğŸ“ˆ **í‚¤ì›Œë“œë³„ ì´ ë…¼ë¬¸ ìˆ˜**:
{chr(10).join(keyword_summaries)}

ğŸ’¡ **ë¶„ì„ ìš”ì•½**:
ì„ íƒí•œ í‚¤ì›Œë“œë“¤ì˜ ì—°êµ¬ íŠ¸ë Œë“œë¥¼ ì‹œê°„ì— ë”°ë¼ ë¹„êµí•˜ì—¬
ê° ì£¼ì œì˜ ê´€ì‹¬ë„ ë³€í™”ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

            else:
                # ê¸°ë³¸ ì„¤ëª…
                papers_count = pubmed_data.get("papers_count", len(papers))
                explanation = f"""ğŸ” ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„

**ë¶„ì„ ì£¼ì œ**: "{query}"

ğŸ“„ ì´ {papers_count}ê°œì˜ ê´€ë ¨ ë…¼ë¬¸ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.

PubMed ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì‹  ì—°êµ¬ ë™í–¥ì„ í™•ì¸í•˜ì„¸ìš”."""
            
            logger.info(f"   âœ… Generated explanation ({len(explanation)} chars)")
            
            state["explanation"] = explanation
            state["status"] = "success"
            state["metadata"]["explanation_length"] = len(explanation)
            
        except Exception as e:
            logger.error(f"   âŒ Explanation generation error: {e}")
            state["error"] = f"ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {str(e)}"
            state["explanation"] = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            state["status"] = "error"
        
        return state
        """
        Node 2: PubMedì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        """
        logger.info("ğŸ” Node 2: Fetching PubMed data...")
        
        try:
            analysis_type = state["analysis_type"]
            keywords = state["keywords"]
            context = state.get("context", {})
            
            pubmed_data = {}
            papers = []
            
            if analysis_type == "temporal_trends":
                # ì‹œê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„
                start_year = context.get("start_year", 2015)
                end_year = context.get("end_year", 2024)
                
                logger.info(f"   Fetching temporal trends ({start_year}-{end_year})...")
                
                trends_data = await self.pubmed.searcher.get_publication_trends_parallel(
                    query=keywords[0],
                    start_year=start_year,
                    end_year=end_year,
                    normalize=True
                )
                
                pubmed_data["trends"] = trends_data
                pubmed_data["start_year"] = start_year
                pubmed_data["end_year"] = end_year
                
                # ìµœê·¼ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°
                papers = await self.pubmed.search(
                    query=keywords[0],
                    max_results=10,
                    sort="pub_date"
                )
                
            elif analysis_type == "geographic_distribution":
                # ì§€ì—­ë³„ ë¶„í¬ ë¶„ì„
                countries = context.get("countries", None)
                
                logger.info(f"   Fetching geographic distribution...")
                
                geo_data = await self.pubmed.searcher.get_geographic_distribution_parallel(
                    query=keywords[0],
                    countries=countries
                )
                
                pubmed_data["geographic"] = geo_data
                
                papers = await self.pubmed.search(
                    query=keywords[0],
                    max_results=10
                )
                
            elif analysis_type == "keyword_comparison":
                # í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„
                start_year = context.get("start_year", 2015)
                end_year = context.get("end_year", 2024)
                
                logger.info(f"   Comparing keywords: {keywords[:4]}")
                
                all_trends = []
                for keyword in keywords[:4]:  # ìµœëŒ€ 4ê°œ
                    trends = await self.pubmed.searcher.get_publication_trends_parallel(
                        query=keyword,
                        start_year=start_year,
                        end_year=end_year,
                        normalize=True
                    )
                    all_trends.append({
                        "keyword": keyword,
                        "data": trends
                    })
                
                pubmed_data["comparisons"] = all_trends
                pubmed_data["start_year"] = start_year
                pubmed_data["end_year"] = end_year
                
                # ì²« ë²ˆì§¸ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ ê²€ìƒ‰
                papers = await self.pubmed.search(
                    query=keywords[0],
                    max_results=10
                )
            
            else:
                # ê¸°ë³¸: ìµœê·¼ ë…¼ë¬¸ ê²€ìƒ‰
                logger.info(f"   Fetching recent papers...")
                papers = await self.pubmed.search(
                    query=keywords[0],
                    max_results=20
                )
                pubmed_data["papers_count"] = len(papers)
            
            logger.info(f"   âœ… Fetched {len(papers)} papers")
            
            state["pubmed_data"] = pubmed_data
            state["papers"] = papers
            state["metadata"]["papers_count"] = len(papers)
            state["status"] = "data_fetched"
            
        except Exception as e:
            logger.error(f"   âŒ PubMed fetch error: {e}")
            state["error"] = f"PubMed ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}"
            state["status"] = "error"
        
        return state

    async def _generate_visualization(self, state: AgentState) -> AgentState:
        """
        Node 3: ì°¨íŠ¸ ì„¤ì • ìƒì„±
        """
        logger.info("ğŸ“ˆ Node 3: Generating visualization...")
        
        try:
            analysis_type = state["analysis_type"]
            pubmed_data = state.get("pubmed_data", {})
            
            chart_config = None
            
            if analysis_type == "temporal_trends" and "trends" in pubmed_data:
                # ì‹œê°„ë³„ íŠ¸ë Œë“œ ì°¨íŠ¸
                trends = pubmed_data["trends"]
                
                chart_config = {
                    "type": "line",
                    "data": {
                        "labels": [str(year) for year in trends["years"]],
                        "datasets": [
                            {
                                "label": "ë…¼ë¬¸ ìˆ˜",
                                "data": trends["counts"],
                                "borderColor": "rgb(59, 130, 246)",
                                "backgroundColor": "rgba(59, 130, 246, 0.1)",
                                "tension": 0.3
                            }
                        ]
                    },
                    "options": {
                        "responsive": True,
                        "plugins": {
                            "title": {
                                "display": True,
                                "text": "ì‹œê°„ë³„ ì—°êµ¬ íŠ¸ë Œë“œ"
                            }
                        }
                    }
                }
                
                # ì •ê·œí™” ë°ì´í„° ì¶”ê°€
                if "normalized_counts" in trends:
                    chart_config["data"]["datasets"].append({
                        "label": "ì •ê·œí™”ëœ ìˆ˜ (per 100K)",
                        "data": trends["normalized_counts"],
                        "borderColor": "rgb(239, 68, 68)",
                        "backgroundColor": "rgba(239, 68, 68, 0.1)",
                        "tension": 0.3,
                        "yAxisID": "y1"
                    })
                
            elif analysis_type == "geographic_distribution" and "geographic" in pubmed_data:
                # ì§€ì—­ë³„ ë¶„í¬ ì°¨íŠ¸
                geo_data = pubmed_data["geographic"]
                
                sorted_countries = sorted(
                    geo_data["countries"].items(),
                    key=lambda x: x[1]["count"],
                    reverse=True
                )[:15]
                
                chart_config = {
                    "type": "bar",
                    "data": {
                        "labels": [country for country, _ in sorted_countries],
                        "datasets": [{
                            "label": "ë…¼ë¬¸ ìˆ˜",
                            "data": [data["count"] for _, data in sorted_countries],
                            "backgroundColor": "rgba(59, 130, 246, 0.7)",
                            "borderColor": "rgb(59, 130, 246)",
                            "borderWidth": 1
                        }]
                    },
                    "options": {
                        "indexAxis": "y",
                        "responsive": True
                    }
                }
                
            elif analysis_type == "keyword_comparison" and "comparisons" in pubmed_data:
                # í‚¤ì›Œë“œ ë¹„êµ ì°¨íŠ¸
                comparisons = pubmed_data["comparisons"]
                
                colors = [
                    'rgb(59, 130, 246)',
                    'rgb(239, 68, 68)',
                    'rgb(34, 197, 94)',
                    'rgb(234, 179, 8)'
                ]
                
                datasets = []
                for i, comp in enumerate(comparisons):
                    datasets.append({
                        "label": comp["keyword"],
                        "data": comp["data"]["normalized_counts"],
                        "borderColor": colors[i % len(colors)],
                        "backgroundColor": colors[i % len(colors)].replace('rgb', 'rgba').replace(')', ', 0.1)'),
                        "tension": 0.3
                    })
                
                chart_config = {
                    "type": "line",
                    "data": {
                        "labels": [str(year) for year in comparisons[0]["data"]["years"]],
                        "datasets": datasets
                    },
                    "options": {
                        "responsive": True,
                        "plugins": {
                            "title": {
                                "display": True,
                                "text": "í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„"
                            }
                        }
                    }
                }
            
            if chart_config:
                logger.info(f"   âœ… Generated {chart_config['type']} chart")
                state["chart_config"] = chart_config
                state["metadata"]["chart_type"] = chart_config["type"]
            else:
                logger.info("   â„¹ï¸ No chart generated")
            
        except Exception as e:
            logger.error(f"   âŒ Visualization error: {e}")
            state["error"] = f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}"
        
        return state

    async def _generate_explanation(self, state: AgentState) -> AgentState:
        """
        Node 4: ë¶„ì„ ê²°ê³¼ ì„¤ëª… ìƒì„±
        """
        logger.info("ğŸ’¬ Node 4: Generating explanation...")
        
        try:
            analysis_type = state["analysis_type"]
            pubmed_data = state.get("pubmed_data", {})
            papers = state.get("papers", [])
            query = state["query"]
            
            explanation = ""
            summary_payload: Dict[str, Any] = {
                "analysis_type": analysis_type,
                "query": query,
                "papers_count": len(papers)
            }
            
            if analysis_type == "temporal_trends" and "trends" in pubmed_data:
                trends = pubmed_data["trends"]
                total_papers = sum(trends["counts"])
                max_idx = trends["counts"].index(max(trends["counts"]))
                peak_year = trends["years"][max_idx]
                peak_count = trends["counts"][max_idx]
                start_year = pubmed_data["start_year"]
                end_year = pubmed_data["end_year"]
                summary_payload["temporal"] = {
                    "total_papers": total_papers,
                    "peak_year": peak_year,
                    "peak_count": peak_count,
                    "start_year": start_year,
                    "end_year": end_year,
                    "trend_points": list(zip(trends["years"], trends["counts"]))
                }
                summary_payload["recent_titles"] = [
                    paper.get("title") for paper in papers[:3]
                ]
                
                explanation = f"""ğŸ“Š ì‹œê°„ë³„ ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„ ({start_year}-{end_year})

ğŸ” **ë¶„ì„ ì£¼ì œ**: "{query}"

ğŸ“ˆ **ì£¼ìš” í†µê³„**:
â€¢ ì „ì²´ ë…¼ë¬¸ ìˆ˜: {total_papers:,}ê°œ
â€¢ ìµœê³  ë°œí–‰ ì—°ë„: {peak_year}ë…„ ({peak_count:,}ê°œ)
â€¢ ë¶„ì„ ê¸°ê°„: {end_year - start_year + 1}ë…„

ğŸ’¡ **íŠ¸ë Œë“œ ìš”ì•½**:
ìµœê·¼ {end_year - start_year + 1}ë…„ê°„ "{query}" ì£¼ì œì˜ ì—°êµ¬ëŠ” ê¾¸ì¤€í•œ ê´€ì‹¬ì„ ë°›ê³  ìˆìœ¼ë©°,
{peak_year}ë…„ì— ê°€ì¥ ë§ì€ ë…¼ë¬¸ì´ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤.

ğŸ“„ ìµœê·¼ ì£¼ìš” ë…¼ë¬¸ {len(papers)}ê°œê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤."""

            elif analysis_type == "geographic_distribution" and "geographic" in pubmed_data:
                geo_data = pubmed_data["geographic"]
                sorted_countries = sorted(
                    geo_data["countries"].items(),
                    key=lambda x: x[1]["count"],
                    reverse=True
                )
                top_country = sorted_countries[0][0] if sorted_countries else "N/A"
                top_count = sorted_countries[0][1]["count"] if sorted_countries else 0
                total_results = geo_data["total_results"]
                summary_payload["geographic"] = {
                    "top_country": top_country,
                    "top_count": top_count,
                    "total_results": total_results,
                    "countries": [
                        {"country": country, "count": data["count"]}
                        for country, data in sorted_countries[:5]
                    ]
                }
                
                explanation = f"""ğŸŒ ì§€ì—­ë³„ ì—°êµ¬ ë¶„í¬ ë¶„ì„

ğŸ” **ë¶„ì„ ì£¼ì œ**: "{query}"

ğŸ“ˆ **ì£¼ìš” í†µê³„**:
â€¢ ì´ ë…¼ë¬¸ ìˆ˜: {total_results:,}ê°œ
â€¢ ìµœë‹¤ ì—°êµ¬ êµ­ê°€: {top_country} ({top_count:,}ê°œ, {top_count/total_results*100:.1f}%)
â€¢ ë¶„ì„ êµ­ê°€ ìˆ˜: {len(sorted_countries)}ê°œ

ğŸ’¡ **ë¶„í¬ ìš”ì•½**:
"{query}" ì£¼ì œëŠ” ì „ ì„¸ê³„ì ìœ¼ë¡œ ì—°êµ¬ë˜ê³  ìˆìœ¼ë©°,
{top_country}ì—ì„œ ê°€ì¥ í™œë°œí•œ ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤."""

            elif analysis_type == "keyword_comparison" and "comparisons" in pubmed_data:
                comparisons = pubmed_data["comparisons"]
                keyword_summaries = []
                comparison_payload = []
                for comp in comparisons:
                    total = sum(comp["data"]["counts"])
                    keyword_summaries.append(f"â€¢ {comp['keyword']}: {total:,}ê°œ")
                    comparison_payload.append({
                        "keyword": comp["keyword"],
                        "total": total,
                        "trend_points": list(zip(comp["data"]["years"], comp["data"]["counts"]))
                    })
                
                start_year = pubmed_data["start_year"]
                end_year = pubmed_data["end_year"]
                summary_payload["comparisons"] = {
                    "period": {"start": start_year, "end": end_year},
                    "keywords": comparison_payload
                }
                
                explanation = f"""ğŸ“Š í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„ ({start_year}-{end_year})

ğŸ” **ë¹„êµ í‚¤ì›Œë“œ**: {len(comparisons)}ê°œ

ğŸ“ˆ **í‚¤ì›Œë“œë³„ ì´ ë…¼ë¬¸ ìˆ˜**:
{chr(10).join(keyword_summaries)}

ğŸ’¡ **ë¶„ì„ ìš”ì•½**:
ì„ íƒí•œ í‚¤ì›Œë“œë“¤ì˜ ì—°êµ¬ íŠ¸ë Œë“œë¥¼ ì‹œê°„ì— ë”°ë¼ ë¹„êµí•˜ì—¬
ê° ì£¼ì œì˜ ê´€ì‹¬ë„ ë³€í™”ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

            else:
                # ê¸°ë³¸ ì„¤ëª…
                papers_count = pubmed_data.get("papers_count", len(papers))
                summary_payload["papers_count"] = papers_count
                summary_payload["recent_titles"] = [
                    paper.get("title") for paper in papers[:5]
                ]
                explanation = f"""ğŸ” ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„

**ë¶„ì„ ì£¼ì œ**: "{query}"

ğŸ“„ ì´ {papers_count}ê°œì˜ ê´€ë ¨ ë…¼ë¬¸ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.

PubMed ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì‹  ì—°êµ¬ ë™í–¥ì„ í™•ì¸í•˜ì„¸ìš”."""
            
            logger.info(f"   âœ… Generated explanation ({len(explanation)} chars)")
            
            explanation = await self._generate_llm_explanation(
                query=query,
                analysis_type=analysis_type,
                summary_payload=summary_payload,
                fallback=explanation
            )

            state["explanation"] = explanation
            state["status"] = "success"
            state["metadata"]["explanation_length"] = len(explanation)
            
        except Exception as e:
            logger.error(f"   âŒ Explanation generation error: {e}")
            state["error"] = f"ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {str(e)}"
            state["status"] = "error"
        
        return state

    async def _generate_llm_explanation(
        self,
        query: str,
        analysis_type: str,
        summary_payload: Dict[str, Any],
        fallback: str
    ) -> str:
        """
        Use OpenAI-compatible client to create a natural-language explanation.
        Falls back to the templated explanation if the LLM call fails.
        """
        try:
            payload_json = json.dumps(summary_payload, ensure_ascii=False, indent=2)
            system_prompt = (
                "ë‹¹ì‹ ì€ ì˜í•™ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. "
                "ì£¼ì–´ì§„ PubMed í†µê³„ ì •ë³´ë¥¼ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. "
                "ìˆ«ì í•´ì„ê³¼ ì˜ë¯¸ë¥¼ í•¨ê»˜ ì„¤ëª…í•˜ê³  í™˜ì/ì—°êµ¬ì ëª¨ë‘ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‘ì„±í•©ë‹ˆë‹¤."
            )
            user_content = (
                f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n"
                f"ë¶„ì„ ìœ í˜•: {analysis_type}\n"
                f"ë°ì´í„° ìš”ì•½(JSON):\n{payload_json}\n\n"
                "ìœ„ ë°ì´í„°ë¥¼ í™œìš©í•´ 3~4ê°œì˜ ë‹¨ë½ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ê³ , "
                "í•µì‹¬ í†µê³„ì™€ ì‹œì‚¬ì ì„ í¬í•¨í•´ì£¼ì„¸ìš”."
            )
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ]
            return await self._chat_completion(messages=messages, temperature=0.2, max_tokens=700)
        except Exception as exc:
            logger.warning(f"LLM explanation failed, using fallback: {exc}")
            return fallback

    def estimate_context_usage(self, user_input: str) -> int:
        """ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        return int(len(user_input) * 1.5) + 500 + 800

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.pubmed.close()
