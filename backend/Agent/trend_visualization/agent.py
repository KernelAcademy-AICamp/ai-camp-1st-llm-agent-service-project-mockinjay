"""
Trend Visualization Agent Implementation
Provides data trend analysis and visualization with PubMed integration
"""

import sys
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging
from datetime import datetime

backend_path = Path(__file__).parent.parent.parent
if str(backend_path) not in sys.path:
    sys.path.insert(0, str(backend_path))

from Agent.base_agent import BaseAgent
from Agent.core.contracts import AgentRequest, AgentResponse
from Agent.api.mongodb_client import MongoDBClient
from Agent.api.pubmed_client import PubMedClient

logger = logging.getLogger(__name__)


class TrendVisualizationAgent(BaseAgent):
    """Data Trend Analysis and Visualization Agent with PubMed Integration"""

    def __init__(self):
        super().__init__(agent_type="trend_visualization")
        self.mongodb = MongoDBClient()
        self.pubmed = PubMedClient()
        self._initialized = False

    async def _initialize(self):
        if not self._initialized:
            await self.mongodb.connect()
            self._initialized = True

    async def process(
        self,
        user_input: str,
        session_id: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Process trend visualization request"""
        await self._initialize()

        request = AgentRequest(
            query=user_input,
            session_id=session_id,
            context=context or {},
            profile=context.get('profile', 'general') if context else 'general',
            language=context.get('language', 'ko') if context else 'ko'
        )

        tokens_estimated = self.estimate_context_usage(user_input)
        self.context_usage += tokens_estimated

        try:
            # Determine analysis type from context
            analysis_type = context.get('analysis_type', 'general') if context else 'general'

            if analysis_type == 'temporal':
                return await self._analyze_temporal_trends(request, context)
            elif analysis_type == 'geographic':
                return await self._analyze_geographic_distribution(request, context)
            elif analysis_type == 'mesh':
                return await self._analyze_mesh_categories(request, context)
            elif analysis_type == 'compare':
                return await self._compare_keywords(request, context)
            else:
                # General analysis - query all data sources
                return await self._analyze_general_trends(request, context)

        except Exception as e:
            logger.error(f"Trend visualization agent error: {e}", exc_info=True)
            return {
                "answer": f"íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "papers": [],
                "tokens_used": 0,
                "status": "error",
                "agent_type": self.agent_type,
                "metadata": {"error": str(e)}
            }

    async def _analyze_temporal_trends(
        self,
        request: AgentRequest,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Analyze publication trends over time"""
        start_year = context.get('start_year', 2015)
        end_year = context.get('end_year', 2024)
        normalize = context.get('normalize', True)

        try:
            # Get temporal trends from PubMed
            trends_data = await self.pubmed.searcher.get_publication_trends_parallel(
                query=request.query,
                start_year=start_year,
                end_year=end_year,
                normalize=normalize
            )

            # Get recent papers
            recent_papers = await self.pubmed.search(
                query=request.query,
                max_results=10,
                sort="pub_date"
            )

            # Generate chart configuration
            chart_config = {
                'type': 'line',
                'data': {
                    'labels': [str(year) for year in trends_data['years']],
                    'datasets': [
                        {
                            'label': 'ë…¼ë¬¸ ìˆ˜' if request.language == 'ko' else 'Paper Count',
                            'data': trends_data['counts'],
                            'borderColor': 'rgb(59, 130, 246)',
                            'backgroundColor': 'rgba(59, 130, 246, 0.1)',
                            'tension': 0.3
                        }
                    ]
                }
            }

            if normalize and 'normalized_counts' in trends_data:
                chart_config['data']['datasets'].append({
                    'label': 'ì •ê·œí™”ëœ ìˆ˜ (per 100K)' if request.language == 'ko' else 'Normalized (per 100K)',
                    'data': trends_data['normalized_counts'],
                    'borderColor': 'rgb(239, 68, 68)',
                    'backgroundColor': 'rgba(239, 68, 68, 0.1)',
                    'tension': 0.3,
                    'yAxisID': 'y1'
                })

            # Generate explanation
            total_papers = sum(trends_data['counts'])
            max_year_idx = trends_data['counts'].index(max(trends_data['counts']))
            peak_year = trends_data['years'][max_year_idx]
            peak_count = trends_data['counts'][max_year_idx]

            explanation = f"""ì‹œê°„ë³„ ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„ ({start_year}-{end_year}):

ðŸ“Š ì „ì²´ ë…¼ë¬¸ ìˆ˜: {total_papers:,}ê°œ
ðŸ“ˆ ìµœê³  ë°œí–‰ ì—°ë„: {peak_year}ë…„ ({peak_count:,}ê°œ)
ðŸ“… ë¶„ì„ ê¸°ê°„: {end_year - start_year + 1}ë…„

ìµœê·¼ {end_year - start_year + 1}ë…„ê°„ "{request.query}" ì£¼ì œì˜ ì—°êµ¬ëŠ” ê¾¸ì¤€í•œ ê´€ì‹¬ì„ ë°›ê³  ìžˆìœ¼ë©°,
{peak_year}ë…„ì— ê°€ìž¥ ë§Žì€ ë…¼ë¬¸ì´ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤."""

            response = AgentResponse(
                answer=explanation,
                sources=[chart_config],
                papers=recent_papers[:5],
                tokens_used=100,
                status="success",
                agent_type=self.agent_type,
                metadata={
                    'total_papers': total_papers,
                    'peak_year': peak_year,
                    'analysis_period': f"{start_year}-{end_year}"
                }
            )

            return response.model_dump()

        except Exception as e:
            logger.error(f"Temporal trends error: {e}", exc_info=True)
            raise

    async def _analyze_geographic_distribution(
        self,
        request: AgentRequest,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Analyze geographic distribution of research"""
        countries = context.get('countries', None)

        try:
            # Get geographic distribution from PubMed
            geo_data = await self.pubmed.searcher.get_geographic_distribution_parallel(
                query=request.query,
                countries=countries
            )

            # Sort countries by count
            sorted_countries = sorted(
                geo_data['countries'].items(),
                key=lambda x: x[1]['count'],
                reverse=True
            )[:15]  # Top 15 countries

            # Generate chart configuration
            chart_config = {
                'type': 'bar',
                'data': {
                    'labels': [country for country, _ in sorted_countries],
                    'datasets': [{
                        'label': 'ë…¼ë¬¸ ìˆ˜' if request.language == 'ko' else 'Paper Count',
                        'data': [data['count'] for _, data in sorted_countries],
                        'backgroundColor': 'rgba(59, 130, 246, 0.7)',
                        'borderColor': 'rgb(59, 130, 246)',
                        'borderWidth': 1
                    }]
                },
                'options': {
                    'indexAxis': 'y',
                    'responsive': True
                }
            }

            # Get sample papers
            papers = await self.pubmed.search(
                query=request.query,
                max_results=10
            )

            # Generate explanation
            top_country = sorted_countries[0][0] if sorted_countries else 'N/A'
            top_count = sorted_countries[0][1]['count'] if sorted_countries else 0
            total_results = geo_data['total_results']

            explanation = f"""ì§€ì—­ë³„ ì—°êµ¬ ë¶„í¬ ë¶„ì„:

ðŸŒ ì´ ë…¼ë¬¸ ìˆ˜: {total_results:,}ê°œ
ðŸ† ìµœë‹¤ ì—°êµ¬ êµ­ê°€: {top_country} ({top_count:,}ê°œ, {top_count/total_results*100:.1f}%)
ðŸ“Š ë¶„ì„ êµ­ê°€ ìˆ˜: {len(sorted_countries)}ê°œ

"{request.query}" ì£¼ì œëŠ” ì „ ì„¸ê³„ì ìœ¼ë¡œ ì—°êµ¬ë˜ê³  ìžˆìœ¼ë©°,
{top_country}ì—ì„œ ê°€ìž¥ í™œë°œí•œ ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìžˆìŠµë‹ˆë‹¤."""

            response = AgentResponse(
                answer=explanation,
                sources=[chart_config],
                papers=papers[:5],
                tokens_used=100,
                status="success",
                agent_type=self.agent_type,
                metadata={
                    'total_results': total_results,
                    'top_country': top_country,
                    'countries_analyzed': len(sorted_countries)
                }
            )

            return response.model_dump()

        except Exception as e:
            logger.error(f"Geographic distribution error: {e}", exc_info=True)
            raise

    async def _analyze_mesh_categories(
        self,
        request: AgentRequest,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Analyze MeSH category distribution"""
        try:
            # Get MeSH distribution from PubMed
            from app.services.pubmed_search import MESH_CATEGORIES, MESH_SUBHEADINGS

            mesh_data = await self.pubmed.searcher.get_mesh_distribution_parallel(
                query=request.query,
                categories=MESH_CATEGORIES[:10],
                subheadings=MESH_SUBHEADINGS[:10]
            )

            # Sort categories
            categories = sorted(
                mesh_data.get('categories', []),
                key=lambda x: x['proportion'],
                reverse=True
            )[:10]

            subheadings = sorted(
                mesh_data.get('subheadings', []),
                key=lambda x: x['proportion'],
                reverse=True
            )[:10]

            # Generate chart configurations
            category_chart = {
                'type': 'doughnut',
                'data': {
                    'labels': [cat['name'] for cat in categories],
                    'datasets': [{
                        'label': 'ì¹´í…Œê³ ë¦¬ ë¶„í¬' if request.language == 'ko' else 'Category Distribution',
                        'data': [cat['count'] for cat in categories],
                        'backgroundColor': [
                            'rgba(59, 130, 246, 0.7)',
                            'rgba(239, 68, 68, 0.7)',
                            'rgba(34, 197, 94, 0.7)',
                            'rgba(234, 179, 8, 0.7)',
                            'rgba(168, 85, 247, 0.7)',
                            'rgba(236, 72, 153, 0.7)',
                            'rgba(20, 184, 166, 0.7)',
                            'rgba(249, 115, 22, 0.7)',
                            'rgba(99, 102, 241, 0.7)',
                            'rgba(132, 204, 22, 0.7)'
                        ]
                    }]
                }
            }

            subheading_chart = {
                'type': 'bar',
                'data': {
                    'labels': [sub['name'] for sub in subheadings],
                    'datasets': [{
                        'label': 'ì„œë¸Œí—¤ë”© ë¶„í¬' if request.language == 'ko' else 'Subheading Distribution',
                        'data': [sub['count'] for sub in subheadings],
                        'backgroundColor': 'rgba(34, 197, 94, 0.7)',
                        'borderColor': 'rgb(34, 197, 94)',
                        'borderWidth': 1
                    }]
                }
            }

            # Get sample papers
            papers = await self.pubmed.search(
                query=request.query,
                max_results=10
            )

            # Generate explanation
            total_results = mesh_data['total_results']
            top_category = categories[0]['name'] if categories else 'N/A'
            top_subheading = subheadings[0]['name'] if subheadings else 'N/A'

            explanation = f"""MeSH ì¹´í…Œê³ ë¦¬ ë¶„ì„:

ðŸ“š ì´ ë…¼ë¬¸ ìˆ˜: {total_results:,}ê°œ
ðŸ·ï¸ ì£¼ìš” ì¹´í…Œê³ ë¦¬: {top_category}
ðŸ” ì£¼ìš” ì„œë¸Œí—¤ë”©: {top_subheading}

"{request.query}" ì£¼ì œëŠ” ì£¼ë¡œ {top_category} ì¹´í…Œê³ ë¦¬ì™€ ê´€ë ¨ì´ ìžˆìœ¼ë©°,
{top_subheading} ê´€ì ì—ì„œ ë§Žì´ ì—°êµ¬ë˜ê³  ìžˆìŠµë‹ˆë‹¤."""

            response = AgentResponse(
                answer=explanation,
                sources=[category_chart, subheading_chart],
                papers=papers[:5],
                tokens_used=100,
                status="success",
                agent_type=self.agent_type,
                metadata={
                    'total_results': total_results,
                    'top_category': top_category,
                    'top_subheading': top_subheading
                }
            )

            return response.model_dump()

        except Exception as e:
            logger.error(f"MeSH category error: {e}", exc_info=True)
            raise

    async def _compare_keywords(
        self,
        request: AgentRequest,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Compare trends across multiple keywords"""
        keywords = context.get('keywords', [request.query])
        start_year = context.get('start_year', 2015)
        end_year = context.get('end_year', 2024)

        try:
            # Get trends for each keyword
            all_trends = []
            for keyword in keywords[:4]:  # Limit to 4 keywords
                trends = await self.pubmed.searcher.get_publication_trends_parallel(
                    query=keyword,
                    start_year=start_year,
                    end_year=end_year,
                    normalize=True
                )
                all_trends.append({
                    'keyword': keyword,
                    'data': trends
                })

            # Generate chart configuration
            colors = [
                'rgb(59, 130, 246)',   # blue
                'rgb(239, 68, 68)',    # red
                'rgb(34, 197, 94)',    # green
                'rgb(234, 179, 8)'     # yellow
            ]

            datasets = []
            for i, trend in enumerate(all_trends):
                datasets.append({
                    'label': trend['keyword'],
                    'data': trend['data']['normalized_counts'],
                    'borderColor': colors[i],
                    'backgroundColor': colors[i].replace('rgb', 'rgba').replace(')', ', 0.1)'),
                    'tension': 0.3
                })

            chart_config = {
                'type': 'line',
                'data': {
                    'labels': [str(year) for year in all_trends[0]['data']['years']],
                    'datasets': datasets
                }
            }

            # Get papers for first keyword
            papers = await self.pubmed.search(
                query=keywords[0],
                max_results=10
            )

            # Generate explanation
            keyword_summaries = []
            for trend in all_trends:
                total = sum(trend['data']['counts'])
                keyword_summaries.append(f"- {trend['keyword']}: {total:,}ê°œ")

            explanation = f"""í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„ ({start_year}-{end_year}):

ðŸ“Š ë¹„êµ í‚¤ì›Œë“œ ìˆ˜: {len(all_trends)}ê°œ
ðŸ“ˆ ê¸°ê°„: {end_year - start_year + 1}ë…„

í‚¤ì›Œë“œë³„ ì´ ë…¼ë¬¸ ìˆ˜:
{chr(10).join(keyword_summaries)}

ì„ íƒí•œ í‚¤ì›Œë“œë“¤ì˜ ì—°êµ¬ íŠ¸ë Œë“œë¥¼ ì‹œê°„ì— ë”°ë¼ ë¹„êµí•˜ì—¬
ê° ì£¼ì œì˜ ê´€ì‹¬ë„ ë³€í™”ë¥¼ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤."""

            response = AgentResponse(
                answer=explanation,
                sources=[chart_config],
                papers=papers[:5],
                tokens_used=100,
                status="success",
                agent_type=self.agent_type,
                metadata={
                    'keywords_compared': len(all_trends),
                    'analysis_period': f"{start_year}-{end_year}"
                }
            )

            return response.model_dump()

        except Exception as e:
            logger.error(f"Keyword comparison error: {e}", exc_info=True)
            raise

    async def _analyze_general_trends(
        self,
        request: AgentRequest,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """General trend analysis using MongoDB data"""
        try:
            all_data = await self.mongodb.search_parallel(
                query=request.query,
                collections=['qa_kidney', 'papers_kidney', 'medical_kidney', 'guidelines_kidney'],
                limit=50
            )

            trends = self._analyze_trends(all_data, request.query)
            chart_config = self._generate_chart_data(trends)
            explanation = self._explain_trends(trends)

            response = AgentResponse(
                answer=explanation,
                sources=[chart_config],
                papers=[],
                tokens_used=100,
                status="success",
                agent_type=self.agent_type,
                metadata={
                    'data_points': len(all_data),
                    'trend_type': trends.get('type', 'general')
                }
            )

            return response.model_dump()

        except Exception as e:
            logger.error(f"General trends error: {e}", exc_info=True)
            raise

    def _analyze_trends(self, data: List[Dict], query: str) -> Dict:
        """Analyze trend patterns from MongoDB data"""
        categories = {}
        for item in data:
            category = item.get('category', 'unknown')
            categories[category] = categories.get(category, 0) + 1

        return {
            'categories': categories,
            'type': 'research' if 'ì—°êµ¬' in query else 'general',
            'total_data': len(data)
        }

    def _generate_chart_data(self, trends: Dict) -> Dict:
        """Generate chart configuration from trends"""
        categories = trends.get('categories', {})
        return {
            'type': 'bar',
            'data': {
                'labels': list(categories.keys())[:10],
                'datasets': [{
                    'label': 'ë°ì´í„° ë¶„í¬',
                    'data': list(categories.values())[:10],
                    'backgroundColor': 'rgba(59, 130, 246, 0.7)',
                    'borderColor': 'rgb(59, 130, 246)',
                    'borderWidth': 1
                }]
            }
        }

    def _explain_trends(self, trends: Dict) -> str:
        """Explain trend findings"""
        total = trends.get('total_data', 0)
        categories = trends.get('categories', {})
        top_category = max(categories.items(), key=lambda x: x[1])[0] if categories else 'N/A'

        return f"""íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼:

ðŸ“Š ì´ {total}ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ ë¶„ì„
ðŸ·ï¸ ì£¼ìš” ì¹´í…Œê³ ë¦¬: {top_category}
ðŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬: {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬

ì°¨íŠ¸ë¥¼ í†µí•´ ë°ì´í„° ë¶„í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤."""

    def estimate_context_usage(self, user_input: str) -> int:
        return int(len(user_input) * 1.5) + 500 + 800

    async def close(self):
        await self.mongodb.close()
        self.pubmed.close()
